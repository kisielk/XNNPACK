// Copyright 2022 Google LLC
//
// This source code is licensed under the BSD-style license found in the
// LICENSE file in the root directory of this source tree.
$assert IN_PTRS in ["MULTI", "REUSE"]
$assert OUT_PTRS in ["MULTI", "SWITCH", "MOV"]
$TILE_SIZE = int(256/SIZE)
$NUM_ITERS = TILE_SIZE.bit_length() - 1

$FLOATX = "float" if SIZE == 32 else "double"
$__M256X = "__m256" if SIZE == 32 else "__m256d"
$_MM_STOREU_PX = "_mm_storeu_ps" if SIZE == 32 else "_mm_storeu_pd"
$_MM_CASTPX_PY = "_mm_castps_pd" if SIZE == 32 else ""
$_MM256_UNPACKLO_PX = "_mm256_unpacklo_ps" if SIZE == 32 else "_mm256_unpacklo_pd"
$_MM256_UNPACKHI_PX = "_mm256_unpackhi_ps" if SIZE == 32 else "_mm256_unpackhi_pd"
$_MM256_MASKLOAD_PX = "_mm256_maskload_ps" if SIZE == 32 else "_mm256_maskload_pd"
$_MM256_PERMUTE_PX = "_mm256_permute2f128_ps" if SIZE == 32 else "_mm256_permute2f128_pd"
$_MM256_STOREU_PX = "_mm256_storeu_ps" if SIZE == 32 else "_mm256_storeu_pd"
$_MM256_UNDEFINED_PX = "_mm256_undefined_ps" if SIZE == 32 else "_mm256_undefined_pd"
$_MM256_CASTPX256_PX128 = "_mm256_castps256_ps128" if SIZE == 32 else "_mm256_castpd256_pd128"

#include <immintrin.h>

#include <assert.h>

#include <xnnpack/common.h>
#include <xnnpack/math.h>
#include <xnnpack/transpose.h>
#include <xnnpack/unaligned.h>

void xnn_x${SIZE}_transposec_ukernel__${TILE_SIZE}x${TILE_SIZE}_${IN_PTRS.lower()}_${OUT_PTRS.lower()}_avx(
    const uint${SIZE}_t* input,
    uint${SIZE}_t* output,
    size_t input_stride,
    size_t output_stride,
    size_t block_width,
    size_t block_height,
    const union xnn_x${SIZE}_transpose_params params[restrict XNN_MIN_ELEMENTS(1)]) XNN_OOB_READS
{
  assert(output_stride >= block_height * sizeof(${FLOATX}));
  assert(input_stride >= block_width * sizeof(${FLOATX}));

  const size_t tile_height = ${TILE_SIZE};
  const size_t tile_width = ${TILE_SIZE};
  const size_t tile_hbytes = tile_height * sizeof(${FLOATX});
  const size_t tile_wbytes = tile_width * sizeof(${FLOATX});
  const size_t input_reset = tile_wbytes - round_down_po2(block_height, tile_height) * input_stride;
  $if IN_PTRS == "MULTI":
    const size_t input_offset = tile_height * input_stride;
  $if OUT_PTRS == "MOV":
    const size_t output_reset = tile_width * output_stride - round_down_po2(block_height, 2) * sizeof(${FLOATX}) - tile_hbytes;
  $else:
    const size_t output_reset = tile_width * output_stride - round_down_po2(block_height, 2) * sizeof(${FLOATX});

  $if IN_PTRS == "MULTI":
    const ${FLOATX}* i0 = (const ${FLOATX}*) input;
    $for N in range(1, TILE_SIZE):
      const ${FLOATX}* i${N} = (const ${FLOATX}*) ((uintptr_t) i${N-1} + input_stride);
  $else:
    const ${FLOATX}* i0 = (const ${FLOATX}*) input;
  $if OUT_PTRS == "MULTI":
    ${FLOATX}* o0 = (${FLOATX}*) output;
  $if OUT_PTRS == "SWITCH":
    ${FLOATX}* o = (${FLOATX}*) output;
  $elif OUT_PTRS != "MULTI":
    ${FLOATX}* o = (${FLOATX}*) ((uintptr_t) output - tile_hbytes);
  $if OUT_PTRS in ["SWITCH", "MOV"]:
    const size_t minus_output_stride = -output_stride;

  do {
    $if OUT_PTRS == "MULTI":
      ${FLOATX}* o1 = (${FLOATX}*) (block_width < 2 ? o0 : (${FLOATX}*) ((uintptr_t) o0 + output_stride));
      $for N in range(2, TILE_SIZE, 2):
        ${FLOATX}* o${N} = (${FLOATX}*) (block_width <= ${N} ? o0 : (${FLOATX}*) ((uintptr_t) o${N-1} + output_stride));
        ${FLOATX}* o${N+1} = (${FLOATX}*) (block_width < ${N+2} ? o0 : (${FLOATX}*) ((uintptr_t) o${N} + output_stride));
    const size_t rem = min(block_width - 1, ${TILE_SIZE-1});
    $if OUT_PTRS == "MOV":
      const size_t oN_stride = rem * output_stride;
      const size_t oN_offset = oN_stride + tile_hbytes;
    $elif OUT_PTRS == "SWITCH":
      const size_t oN_stride = rem * output_stride;

    __m256i vmask = _mm256_loadu_si256((const __m256i*) ((uintptr_t) &params->avx.mask_table[${TILE_SIZE-1} - rem]));

    size_t bh = block_height;
    for (; bh >= ${TILE_SIZE}; bh -= ${TILE_SIZE}) {
      $for N in range(TILE_SIZE):
        $if IN_PTRS == "REUSE":
          const ${__M256X} v${NUM_ITERS}_${N} = ${_MM256_MASKLOAD_PX}(i0, vmask);
          i0 = (${FLOATX}*) ((uintptr_t) i0 + input_stride);
        $else:
          const ${__M256X} v${NUM_ITERS}_${N} = ${_MM256_MASKLOAD_PX}(i${N}, vmask);
          i${N} = (${FLOATX}*) ((uintptr_t) i${N} + input_offset);

      $for M in range(0, NUM_ITERS-1):
        $for N in range(2):
          $for O in range(TILE_SIZE>>2):
            const ${__M256X} v${NUM_ITERS-M-1}_${N*(TILE_SIZE>>1)+O*2} =  ${_MM256_UNPACKLO_PX}(v${NUM_ITERS-M}_${O+N*(TILE_SIZE>>1)}, v${NUM_ITERS-M}_${(O+N*(TILE_SIZE>>1))+int(TILE_SIZE/4)});
            const ${__M256X} v${NUM_ITERS-M-1}_${N*(TILE_SIZE>>1)+O*2+1} = ${_MM256_UNPACKHI_PX}(v${NUM_ITERS-M}_${O+N*(TILE_SIZE>>1)}, v${NUM_ITERS-M}_${(O+N*(TILE_SIZE>>1))+int(TILE_SIZE/4)});

      $if OUT_PTRS != "SWITCH":
        $for N in range(0,(TILE_SIZE>>1)):
           const ${__M256X} v0_${(N)} = ${_MM256_PERMUTE_PX}(v1_${N}, v1_${N+(TILE_SIZE>>1)}, 0x20);
           const ${__M256X} v0_${(N)+(TILE_SIZE>>1)} = ${_MM256_PERMUTE_PX}(v1_${N}, v1_${N+(TILE_SIZE>>1)}, 0x31);

      $if OUT_PTRS == "SWITCH":
        ${FLOATX}* oN = (${FLOATX}*) ((uintptr_t) o + oN_stride);
        switch (rem) {
          default:
            XNN_UNREACHABLE;
          $for N in reversed(range(TILE_SIZE>>1, TILE_SIZE)):
            case ${N}: {
              const ${__M256X} v0_${N} = ${_MM256_PERMUTE_PX}(v1_${N-(TILE_SIZE>>1)}, v1_${N}, 0x31);
              ${_MM256_STOREU_PX}(oN, v0_${N});
              oN = (${FLOATX}*) ((uintptr_t) oN + minus_output_stride);
            }
          $for N in reversed(range(2, TILE_SIZE>>1)):
            case ${N}: {
              const ${__M256X} v0_${(N)} = ${_MM256_PERMUTE_PX}(v1_${N}, v1_${N+(TILE_SIZE>>1)}, 0x20);
              ${_MM256_STOREU_PX}(oN, v0_${N});
              oN = (${FLOATX}*) ((uintptr_t) oN + minus_output_stride);
            }
          case 1: {
            const __m256 v0_1 = ${_MM256_PERMUTE_PX}(v1_1, v1_${(TILE_SIZE>>1)+1}, 0x20);
            ${_MM256_STOREU_PX}( oN, v0_1);
          }
          case 0: {
            const __m256 v0_0 = ${_MM256_PERMUTE_PX}(v1_0, v1_${TILE_SIZE>>1}, 0x20);
            ${_MM256_STOREU_PX}(o, v0_0);
            o = (${FLOATX}*) ((uintptr_t) o + tile_hbytes);
          }
        }
      $elif OUT_PTRS == "MOV":
        o = (${FLOATX}*) ((uintptr_t) o + oN_offset);
        ${_MM256_STOREU_PX}(o, v0_${TILE_SIZE-1});
        ${FLOATX} *oN = (${FLOATX}*) ((uintptr_t) o + minus_output_stride);
        $for N in reversed(range(2, TILE_SIZE, 2)):
          if XNN_UNPREDICTABLE(block_width > ${N+1}) {
            o = oN;
          }
          ${_MM256_STOREU_PX}(o, v0_${N});
          oN = (${FLOATX}*) ((uintptr_t) o + minus_output_stride);
          if XNN_UNPREDICTABLE(block_width >= ${N+1}) {
            o = oN;
          }
          ${_MM256_STOREU_PX}(o, v0_${N-1});
          oN = (${FLOATX}*) ((uintptr_t) o + minus_output_stride);
        if XNN_UNPREDICTABLE(block_width > 1) {
          o = oN;
        }
        ${_MM256_STOREU_PX}(o, v0_0);
      $else:
        $for N in reversed(range(TILE_SIZE)):
          ${_MM256_STOREU_PX}(o${N}, v0_${N});
          o${N} = (${FLOATX}*) ((uintptr_t) o${N} + tile_hbytes);
    }
    $if OUT_PTRS == "MOV":
      o = (${FLOATX}*) ((uintptr_t) o + tile_hbytes);
    if (bh != 0) {
      $if IN_PTRS == "REUSE":
        const ${__M256X} v${NUM_ITERS}_0 = ${_MM256_MASKLOAD_PX}(i0, vmask);
        $for N in range(1, TILE_SIZE - 1, 2):
          const ${FLOATX} *i${N} = (const ${FLOATX}*) ((uintptr_t) i${N-1} + input_stride);
          if XNN_UNPREDICTABLE(bh < ${N+1}) {
            i${N} = i${N-1};
          }
          const ${__M256X} v${NUM_ITERS}_${N} = ${_MM256_MASKLOAD_PX}(i${N}, vmask);
          const ${FLOATX} *i${N+1} = (const ${FLOATX}*) ((uintptr_t) i${N} + input_stride);
          if XNN_UNPREDICTABLE(bh <= ${N+1}) {
            i${N+1} = i${N};
          }
          const ${__M256X} v${NUM_ITERS}_${N+1} = ${_MM256_MASKLOAD_PX}(i${N+1}, vmask);
      $else:
        const ${__M256X} v${NUM_ITERS}_0 = ${_MM256_MASKLOAD_PX}(i0, vmask);
        $for N in range(1, TILE_SIZE - 1, 2):
          if XNN_UNPREDICTABLE(bh < ${N+1}) {
            i${N} = i0;
          }
          const ${__M256X} v${NUM_ITERS}_${N} = ${_MM256_MASKLOAD_PX}(i${N}, vmask);
          if XNN_UNPREDICTABLE(bh <= ${N+1}) {
            i${N+1} = i0;
          }
          const ${__M256X} v${NUM_ITERS}_${N+1} = ${_MM256_MASKLOAD_PX}(i${N+1}, vmask);
      const ${__M256X} v${NUM_ITERS}_${TILE_SIZE-1} = ${_MM256_UNDEFINED_PX}();

      $for M in range(0, NUM_ITERS-1):
        $for N in range(2):
          $for O in range(TILE_SIZE>>2):
            const ${__M256X} v${NUM_ITERS-M-1}_${N*(TILE_SIZE>>1)+O*2} =  ${_MM256_UNPACKLO_PX}(v${NUM_ITERS-M}_${O+N*(TILE_SIZE>>1)}, v${NUM_ITERS-M}_${(O+N*(TILE_SIZE>>1))+int(TILE_SIZE/4)});
            const ${__M256X} v${NUM_ITERS-M-1}_${N*(TILE_SIZE>>1)+O*2+1} = ${_MM256_UNPACKHI_PX}(v${NUM_ITERS-M}_${O+N*(TILE_SIZE>>1)}, v${NUM_ITERS-M}_${(O+N*(TILE_SIZE>>1))+int(TILE_SIZE/4)});

      $for N in range(0,(TILE_SIZE>>1)):
         ${__M256X} v0_${(N)} = ${_MM256_PERMUTE_PX}(v1_${N}, v1_${N+(TILE_SIZE>>1)}, 0x20);
         ${__M256X} v0_${(N)+(TILE_SIZE>>1)} = ${_MM256_PERMUTE_PX}(v1_${N}, v1_${N+(TILE_SIZE>>1)}, 0x31);

      if (bh & ${TILE_SIZE>>1}) {
        $if OUT_PTRS == "SWITCH":
          ${FLOATX}* oN = (${FLOATX}*) ((uintptr_t) o + oN_stride);
          switch (rem) {
            $for N in reversed(range(2, TILE_SIZE)):
              case ${N}:
                ${_MM_STOREU_PX}(oN, ${_MM256_CASTPX256_PX128}(v0_${N}));
                oN = (${FLOATX}*) ((uintptr_t) oN + minus_output_stride);
            case 1:
              ${_MM_STOREU_PX}(oN, ${_MM256_CASTPX256_PX128}(v0_1));
            case 0:
              ${_MM_STOREU_PX}(o, ${_MM256_CASTPX256_PX128}(v0_0));
              break;
            default:
              XNN_UNREACHABLE;
          }
          o += ${TILE_SIZE>>1};
        $elif OUT_PTRS == "MOV":
          o = (${FLOATX}*) ((uintptr_t) o + oN_stride);
          ${_MM_STOREU_PX}(o, ${_MM256_CASTPX256_PX128}(v0_${TILE_SIZE-1}));
          ${FLOATX} *oN = (${FLOATX}*) ((uintptr_t) o + minus_output_stride);
          $for N in reversed(range(2, TILE_SIZE, 2)):
            if XNN_UNPREDICTABLE(block_width > ${N+1}) {
              o = oN;
            }
            ${_MM_STOREU_PX}(o, ${_MM256_CASTPX256_PX128}(v0_${N}));
            oN = (${FLOATX}*) ((uintptr_t) o + minus_output_stride);
            if XNN_UNPREDICTABLE(block_width >= ${N+1}) {
              o = oN;
            }
            ${_MM_STOREU_PX}(o, ${_MM256_CASTPX256_PX128}(v0_${N-1}));
            oN = (${FLOATX}*) ((uintptr_t) o + minus_output_stride);
          if XNN_UNPREDICTABLE(block_width > 1) {
            o = oN;
          }
          ${_MM_STOREU_PX}(o, ${_MM256_CASTPX256_PX128}(v0_0));
          o += ${TILE_SIZE>>1};
        $else:
          $for N in reversed(range(TILE_SIZE)):
            ${_MM_STOREU_PX}(o${N}, ${_MM256_CASTPX256_PX128}(v0_${N}));
            o${N} += ${TILE_SIZE>>1};
        $for N in range(TILE_SIZE):
          v0_${N} = ${_MM256_PERMUTE_PX}(v0_${N}, v0_${N}, 0x1);
      }

      if (bh & ${TILE_SIZE>>2}) {
        $if OUT_PTRS == "SWITCH":
          ${FLOATX}* oN = (${FLOATX}*) ((uintptr_t) o + oN_stride);
          switch (rem) {
            $for N in reversed(range(2, TILE_SIZE)):
              case ${N}:
                _mm_storel_pd((double*) oN, ${_MM_CASTPX_PY}(${_MM256_CASTPX256_PX128}(v0_${N})));
                oN = (${FLOATX}*) ((uintptr_t) oN + minus_output_stride);
            case 1:
              _mm_storel_pd((double*) oN, ${_MM_CASTPX_PY}(${_MM256_CASTPX256_PX128}(v0_1)));
            case 0:
              _mm_storel_pd((double*) o, ${_MM_CASTPX_PY}(${_MM256_CASTPX256_PX128}(v0_0)));
              break;
            default:
              XNN_UNREACHABLE;
          }
          $if SIZE == 32:
            o += ${TILE_SIZE>>2};
        $elif OUT_PTRS == "MOV":
          o = (${FLOATX}*) ((uintptr_t) o + oN_stride);
          _mm_storel_pd((double*) o, ${_MM_CASTPX_PY}(${_MM256_CASTPX256_PX128}(v0_${TILE_SIZE-1})));
          ${FLOATX} *oN = (${FLOATX}*) ((uintptr_t) o + minus_output_stride);
          $for N in reversed(range(2, TILE_SIZE, 2)):
            if XNN_UNPREDICTABLE(block_width > ${N+1}) {
              o = oN;
            }
            _mm_storel_pd((double*) o, ${_MM_CASTPX_PY}(${_MM256_CASTPX256_PX128}(v0_${N})));
            oN = (${FLOATX}*) ((uintptr_t) o + minus_output_stride);
            if XNN_UNPREDICTABLE(block_width >= ${N+1}) {
              o = oN;
            }
            _mm_storel_pd((double*) o, ${_MM_CASTPX_PY}(${_MM256_CASTPX256_PX128}(v0_${N-1})));
            oN = (${FLOATX}*) ((uintptr_t) o + minus_output_stride);
          if XNN_UNPREDICTABLE(block_width > 1) {
            o = oN;
          }
          _mm_storel_pd((double*) o, ${_MM_CASTPX_PY}(${_MM256_CASTPX256_PX128}(v0_0)));
          $if SIZE == 32:
            o += ${TILE_SIZE>>2};
        $else:
          $for N in reversed(range(TILE_SIZE)):
            _mm_storel_pd((double*) o${N}, ${_MM_CASTPX_PY}(${_MM256_CASTPX256_PX128}(v0_${N})));
            $if SIZE == 32:
              o${N} += ${TILE_SIZE>>2};
        $if SIZE == 32:
          $for N in range(TILE_SIZE):
            v0_${N} = _mm256_castpd_ps(_mm256_unpackhi_pd(_mm256_castps_pd(v0_${N}), _mm256_castps_pd(v0_${N})));
      }
      $if SIZE == 32:
        if (bh & 1) {
          $if OUT_PTRS == "SWITCH":
            ${FLOATX}* oN = (${FLOATX}*) ((uintptr_t) o + oN_stride);
            switch (rem) {
              $for N in reversed(range(2, TILE_SIZE)):
                case ${N}:
                  unaligned_store_u32(oN, _mm256_cvtsi256_si32(_mm256_castps_si256(v0_${N})));
                  oN = (${FLOATX}*) ((uintptr_t) oN + minus_output_stride);
              case 1:
                unaligned_store_u32(oN, _mm256_cvtsi256_si32(_mm256_castps_si256(v0_1)));
              case 0:
                unaligned_store_u32(o, _mm256_cvtsi256_si32(_mm256_castps_si256(v0_0)));
                break;
              default:
                XNN_UNREACHABLE;
            }
          $elif OUT_PTRS == "MOV":
            o = (${FLOATX}*) ((uintptr_t) o + oN_stride);
            unaligned_store_u32(o, _mm256_cvtsi256_si32(_mm256_castps_si256(v0_${TILE_SIZE-1})));
            ${FLOATX}* oN = (${FLOATX}*) ((uintptr_t) o + minus_output_stride);
            $for N in reversed(range(2, TILE_SIZE, 2)):
              if XNN_UNPREDICTABLE(block_width > ${N+1}) {
                o = oN;
              }
              unaligned_store_u32(o, _mm256_cvtsi256_si32(_mm256_castps_si256(v0_${N})));
              oN = (${FLOATX}*) ((uintptr_t) o + minus_output_stride);
              if XNN_UNPREDICTABLE(block_width >= ${N+1}) {
                o = oN;
              }
              unaligned_store_u32(o, _mm256_cvtsi256_si32(_mm256_castps_si256(v0_${N-1})));
              oN = (${FLOATX}*) ((uintptr_t) o + minus_output_stride);
            if XNN_UNPREDICTABLE(block_width > 1) {
              o = oN;
            }
            unaligned_store_u32(o, _mm256_cvtsi256_si32(_mm256_castps_si256(v0_0)));
          $else:
            $for N in reversed(range(TILE_SIZE)):
              unaligned_store_u32(o${N}, _mm256_cvtsi256_si32(_mm256_castps_si256(v0_${N})));
        }
    }

    $if IN_PTRS == "MULTI":
      i0 = (const ${FLOATX}*) ((uintptr_t) i0 + input_reset);
      $for N in range(1, TILE_SIZE):
        i${N} = (const ${FLOATX}*) ((uintptr_t) i${N-1} + input_stride);
    $else:
      i0 = (const ${FLOATX}*) ((uintptr_t) i0 + input_reset);
    $if OUT_PTRS == "MULTI":
      o0 = (${FLOATX}*) ((uintptr_t) o0 + output_reset);
    $else:
      o = (${FLOATX}*) ((uintptr_t) o + output_reset);
    block_width = doz(block_width, tile_width);
  } while (block_width != 0);
}
